<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>A Divide and Conquer Approach to High Dimensional Bayesian Factor Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gautam Sabnis" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="animate.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# A Divide and Conquer Approach to High Dimensional Bayesian Factor Models
### Gautam Sabnis
### Boston University

---


class:animated, slideInRight
# Outline

- Overview
- Covariance Matrix Estimation
  - Estimators for the covariance matrix 
- Factor Models
  - Sparse factor models
  - Bayesian sparse fatent models
- Choice of Priors
- Posterior Computation: MCMC sampler
- A Divide and Conquer Framework
  - Divide
  - Fit
  - Conquer
- Synthetic Data Analysis
- Application to Thyroid Gene Expression Data

???

- Thanks for having me! In this talk, I will present a distributed computing approach for estimating massive covariance matrices via high dimensional Bayesian factor models.

- A key computational challenge encountered by existing methods for covariance matrix estimation in this frameowrk is that they require storage of large matrices in memory and repeated, computationally expensive matrix operations. 

- This limits the scalability of these approaches in applications even with moderate dimensions. 

- The main goal of this work is to have a modeling framework to remove the computational bottlenecks that arise in posterior inference in these models. 


Outline 

Here's the outline of my talk. 

First I will give a brief overview, introduce the covariance matrix problem, introduce factor models and its variants and how they can be used to estimate covariance matrices. I will set up the Bayesian framework with appropriate priors and describe the MCMC sampler for doing inferences in this framework. We will see why the existing methods can't scale well. Next, I will introduce our Divide-and-Conquer modeling framework and show some simulation studies to convince you about the advantages of our approach. I will conclude with an application to a thyroid gene expression dataset. 

---

class: animated, fadeInDown
#Motivation

- Brain spectra `\({\color{blue}{\text{covariance matrix}}}\)` for autism infected adults at the National Taiwan University Hospital. 

&lt;img src="autism.jpg" width="50%" style="display: block; margin: auto;" /&gt;

- Understand the patterns. 

???

Here we have the brain spectra covariance matrix for 30 adults with autism spectrum disorder. The study consists of around 8000 genes that are measured on these 30 indviduals. The goal is to understand how the genes are coexpressed or how they covary with each other. In order to do that, we need the covariance matrix. In the picture here, I am showing you the estimated covariance matrix for the first 110 genes. 

As you can see, there is some structure here. Some genes don't covary with each other as much as the other ones do and their covariance can be assumed to be zero. 

So the idea is to estimate the entries of this matrix from the samples we observe. 
This is an example of high-dimensional data where we have fewer samples than the number of variables. 

---

class: animated, fadeInDown
# Overview

- High-dimensional data is ubiquitous in modern applications

- Example: Omics study where `\(p =\)` biological variables, `\(n =\)` number of samples

- High-dimensional data: `\(p \gg n\)` 

- `\({\color{blue}{\text{Curse of dimensionality}}}\)`: classical statistical methods break down in such settings 

- Dimensionality reduction is crucial


???

High-dimensional data is common in many modern applications. For example, in an omics study, data on a large number of biological variables is gathered for a small number of samples. 

This means that the observations are represented in `\(p-\)`dimensional spaces where `\(p\)` can range from serveral hundreds to millions. 

The curse of dimensionality relates to the fact that classical statistical methods generally breakdown in these settings. 

The goal of dimension reduction is to identify a set of new variables using a linear combination of original variables such that the number of new variables is much smaller than `\(p\)`. 

---

class:animated, fadeInDown

# Covariance Matrix Estimation 
- Goal: estimate a covariance matrix `\(\Sigma \in \mathbb{R}^{p \times p}\)`

- given i.i.d. samples `\(y_i \sim N(0, \Sigma)\)`, for `\(i = \ldots, n\)`

**Why covariance estimation?** 
- Principal Component Analysis (PCA)
- Linear/Quadratic Discriminant analysis (LDA/QDA)
- Generalized Method of Moments (GMM)
- Generalized Least Squares (GLS)
- Canonical Correlation Analysis (CCA)



???


- A covariance matrix is a matrix whose diagonal entries contain the variances of each variable and the off-diagonal entries contrain the covariances each pair of variables. The covariance matrix measures the degree to which each pair of these variables is linearly associated. 

- Given `\(n\)` i.i.d. copies of a p-dimensional Gaussian random vector, we wish to estimate `\(\Sigma\)`. 

- Why covariance estimation? Covariance matrices play an important role in many dimension reduction techniques and estimation procedures in statistics and machine learning. Here is a list of some prominant cases. 

- For example, in PCA, we want to find a low-dimensional coordinate system to project the data in such a way that the variance is maximized. This is done by applying a SVD to the covariance matrix. 

- LDA analysis is a classification algorithm based on modeling class distributions as multivariate Gaussian distributions which requires the estimation of covariance matrices. 

- GMM is a method for estimating parameters in a statistical model using moment conditions. Precise covariance estimates increase the efficiency of the GMM. 

- In GLS, we allow the errors to be correlated and the estimation of the parameters then requires the estimation of noise covariance matrix. 

- CCA finds maximally correlated linear combinations of two vector valued random variables. This relies on covariances and cross-covariances. 

---

class:animated, fadeInDown
# Estimators for Covariance Matrix

**Classical approach** 

Estimate `\(\Sigma\)` via sample covariance matrix
`$$\hat\Sigma_{\text{n}} = \frac{1}{n}\sum\limits_{i=1}^{n}{{y}}_i{{y}}_i^T$$`
The performance of `\(\hat\Sigma_{\text{n}}\)` degrades as `\(c = n/p \rightarrow 0\)`.


**Structured Estimation**

Great interest in regularized estimation `\(({\color{blue}{\text{Ledoit &amp; Wolf, 2003b; Bickel &amp; Levina 2008 a,b; Cai and Zhou, 2011 ...}}})\)`

Low intrinsic dimensionality `\(({\color{blue}{\text{Johnstone, 2001; Bhattacharya &amp; Dunson, 2011; Fan et al., 2013 ...}}})\)`



???
The standard estimator of the population covariance matrix is the sample covariance matrix. The properties of SCM depend on the dimensionality `\(p\)` and the performance of SCM deteriorates as `\(c \rightarrow  0\)`. 

This is because an unstructured matrix has too many degrees of freedom.

There is a lot of interest in regularized estimation where they assume a structure (motivated by the application at hand) that reduces the degrees of freedom. 

An alternative approach to reducing the degrees of freedom is through low intrinsic dimensionality. We will see how this is done in the next few slides. 

---

class:animated, fadeInDown
# Factor Models

- Highly successful approach for dimensionality reduction

- Relate high-dimensional `\({\bf{y}}\)` to `\({\bf{\eta}} \in \mathbb{R}^{k}\)` through
`$${\bf{y_i}} = \Lambda{\bf{\eta_i}} + {\bf{\epsilon_i}}, \quad {\bf{\epsilon_i}} \sim N(0,\Omega_p)$$`

- `\(\Lambda \in \mathbb{R}^{p \times k}\)` is a tall-skinny (k &lt;&lt; p)               `\({\color{blue}{\text{factor loadings}}}\)` matrix 

- `\(\mathbb{E}(\eta) = 0\)`, `\(\text{Var}(\eta) = \mathrm{I}_k\)`

- Assumptions: 1) `\(\Omega\)` is a diagonal matrix, 2) `\(\epsilon\)` is independent of `\(\eta\)`. 

???

Factor models provide a particularly useful framework for reducing the dimensionality. 

Factor models relate `\(y\)` to `\(\eta\)` via this regression equation. `\(\Lambda\)` is the matrix of regression coefficients, but in the context of these models, these coefficients are known as factor loadings. It's a tall and skinny matrix which means the number of rows is much larger than the number of columns. 

Since the factors are unobserved, we assume that they occur in standardized form with mean 0 and variance 1. 

We make 2 assumptions in this model. 

The noise terms are uncorrelated with each other which means `\(\Omega\)` is a diagonal matrix. 
The error/noise terms are uncorrelated with the factors `\(\eta\)`.  

But the more important conceptual assumption is that some underlying structure does exist in these observed `\(y\)` vectors which will be captured by these factors and help explain the dependencies in `\(y\)`. 

  
---

class: middle, animated, fadeInDown

With assumptions (1) and (2), 

`$$\quad \Sigma = \Lambda\Lambda^T + \Omega$$`

which leads to dimension reduction 
`$$\mathcal{O}(p^2) \longrightarrow \mathcal{O}(pk + p)$$` 

???
Under these assumptions, marginalizing out the factors gives us a matrix factorization for `\(\Sigma\)` that can be written as a sum of a low-rank plus a sparse matrix. 

The problem of estimating `\(\Sigma\)` is reduced to estimating estimating two matrices, namely, `\(\Lambda\)` and `\(\Omega\)`. This leads to dimensionality reductions as the number of entries to be estimated are reduced from `\(p^2\)` on the left to `\(pk + p\)` on the right. 

Low intrinsic dimensionality posits that the dependencies between the variables are captured by a small number of latent components called factors that seperate the common sources of variation from variable sepecific noise. 

Although the original specification of factor model leads to dimensionality reduction, the problem is still challenging when `\(p\)` is very large. 


---

class: animated, fadeInDown
#Sparse Factor Models

- A *sparse factor model* `\(\color{blue}{\text{(West 2003)}}\)`
is a factor model in which `\(\Lambda\)` is sparse. 

`$$\Lambda_{p \times k} = {\begin{pmatrix} 
\lambda_{11} &amp; {\color{blue}{0}} &amp; {{\lambda_{13}}} &amp; \ldots &amp; {\color{blue}{0}} \\
\lambda_{21} &amp; {{\lambda_{22}}} &amp; {\color{blue}{0}} &amp; \ldots &amp; {\color{blue}{0}} \\ 
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
{\color{blue}{0}} &amp; \ldots &amp; \lambda_{jh} &amp; \ldots &amp; {\color{blue}{0}} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
{\color{blue}{0}} &amp; {\color{blue}{0}} &amp; \lambda_{p3} &amp; \ldots &amp; {\color{blue}{0}} \\
\end{pmatrix}}$$`

- For example, 
&gt; `\(y =\)` gene expression, `\(\eta =\)` pathway expression 

???

Sparse factor models have been defined to address this issue. 

The intuition comes from high-dimensional genetic applications where factors (the columns) are pathway expressions and only a few genes get loaded onto each pathway giving rise to many 0s. 

---

class: animated, fadeInDown
# Bayesian Sparse Factor Models

####Setup 

- Observe `\(y_i \sim N(0,\Sigma), \; i = 1,\ldots,n\)`
- `\(\eta \sim N(0,\mathrm{I}_k)\)`
- `\(\Lambda \sim\)` Prior distribution
- `\(\Omega \sim\)` Prior distribution

####Posterior distribution 

`$$\underbrace{\Pi(\Lambda, \eta, \Omega \mid y)}_{posterior} \propto \prod\limits_{i=1}^{n}\underbrace{\text{N}(y_i \mid \Lambda\eta_i, \Omega)}_{Likelihood} \underbrace{\pi(\Lambda)\pi(\eta_i)\pi(\Omega)}_{priors}$$`
???

We observe data in the form of `\(p\)`-dimensional vectors. As Bayesians, we assume that parameters in our model `\(\Lambda\)` and `\(\Omega\)` are random variables themselves, and they can be sampled from known statistical distributions called the 'prior' distributions. These distributions represent our assumptions about the model parameters. 

Inferences in any Bayesian model are based on being able to draw random samples from the posterior distribution which is the product of the actual data and the prior distributions. 

These random samples can be used to estimate a distribution of the possible parameter values. 

Please note that I am slightly abusing the notation here. These distributions are placed on each entry of these matrices and not on the matrices as a whole. So these are not matrix-valued distributions. 

---

class: animated, fadeInDown
# Choice of Priors

- Multiplicative Gamma Process Shrinkage Prior (MGPS) &lt;span style="color:blue"&gt;(Bhattacharya &amp; Dunson 2011)&lt;span&gt;

`$$\lambda_{jh}\mid{\phi_{jh}},{\tau_{h}} \sim N(0, (\phi_{jh}\tau_{h})^{-1}), \quad \phi_{jh} \sim \Gamma(\nu/2, \nu/2), \quad \tau_{h} = {\displaystyle\prod\limits_{l=1}^{h}{\delta_{l}}} \notag \\
\delta_1 \sim \Gamma(a_1, 1), \quad \delta_{l} \sim \Gamma(a_2, 1), \; l \ge 2, \quad {\omega_{j}^{-2}} \sim \Gamma(a_\omega, b_\omega)$$`

where 
  - `\(\tau_{h}\)`: global shrinkage parameter
  - `\(\phi_{jh}\)`: local shrinkage parameter
  - `\(\Omega = \text{diag}(\omega_1^2, \ldots, \omega_p^2)\)`
  
???

`\(\lambda_{jh}\)` is the entry in the `\(j^{th}\)` row and `\(h^{th}\)` column of `\(\Lambda\)` with a Gaussian prior distribution with mean 0 and a variance that is controlled by two parameters.

The global shrinkage parameter controls the number of columns in `\(\Lambda\)` while the local shrinkage parameter controls the number of zeros in each column of `\(\Lambda\)`. 

Note that the effect of `\(\tau\)` parameter will be larger on the column that is further to the right in the matrix. 

Wet put a Gamma prior on inverse of specific/noise variances. This is a standard prior. 

---

class: animated, fadeInDown
# Posterior Computation

####MCMC Sampler
If the current state is `\((\Lambda_t, \eta_t, \Omega_t)\)` at the `\(t^{\text{th}}\)` iteration, then update the parameter values by cycling through the following steps, 

- Sample `\(\eta_{(t+1)} \sim N_k(\mu_\eta,\Sigma_\eta \mid \Lambda_t, \Omega_t)\)`
- Sample `\(\Lambda_{j_{(t+1)}} \sim N_k(\mu_\Lambda,\Sigma_\Lambda \mid \eta_{(t+1)}, \Omega_t)\)`
- Sample `\(\omega^{-2}_{(t+1)} \sim \Gamma(a_\omega, b_\omega \mid \Lambda_{(t+1)}, \eta_{(t+1)})\)`
- Sample `\(\Phi_{j_{(t+1)}} \sim \Gamma(a_\phi, b_\phi \mid \Lambda_{(t+1)}, \delta_t)\)`
- Sample `\(\delta_{(t+1)} \sim \Gamma(a_\delta, b_\delta \mid \Lambda_{(t+1)}, \Phi_{(t+1)})\)`
- Sample the hyperparameters `\(a_1\)` and `\(a_2\)` using a Metropolis-Hastings step within the sampler. 

???

As I said earlier, inferences in a Bayesian model are based on our ability to draw samples of parameters from the posterior distribution. To draw these samples, we cycle through the following steps of Gibbs sampler. 

Read off the slide and explain everything in detail. For example, we draw from k-dimensional Gaussian distribution with parameters `\(\mu_\eta\)` and `\(\Sigma_\eta\)` which depend on the parameters `\(\Lambda\)` and `\(\Omega\)` from the previous iteration. 

So you see how the sampler has to store these huge matrices from the previous iterations for updating the parameters in the current iteration. 

If we do this long enough, then eventually we would be drawing samples from the target posterior distribution.  

---

class:animated, fadeInDown
# Computational &amp; Storage Constraints

The MCMC sampler, at each iteration, 

- performs several matrix operations such as matrix multiplication, matrix inversion, Cholesky decomposition
- stores `\(\Lambda\)` for updating `\(\eta\)` and vice versa 

| Matrix Operation        | Example          | Complexity  |
|---------------|:-------------:|------:|
| Multiplication     | `\(\Lambda^{T}\Lambda\)` | `\(\mathcal{O}(pk^2)\)` |
| Inversion      |   `\((\Lambda^{T}\Lambda)^{-1}\)`    |   `\(\mathcal{O}(k^3)\)` |
| Cholesky Decomposition | `\(\Lambda^{T}\Lambda\)`    |    `\(\mathcal{O}(k^3)\)` |

&gt; One iteration of the Gibbs sampler for estimating `\(\Sigma\)` requires `\({O}(k^3 + npk + nk^2 + pk^2)\)` floating point operations for matrix computations, and a storage complexity of `\({O}(pk + k^2)\)`.

???

Despite having to sample from standard distribution such as Gaussian and Gamma, we are sampling from multivariate distributions. Sampling from multivariate distributions requires doing matrix operations such as multiplication, inversion and cholesky decomposition at every step of the sampler. 

So when `\(p = O(10^4)\)` and `\(k = O(10^2)\)`, some of the methods cited earlier do not scale. This is our motivation for proposing a Divide and conquer strategy for covariance matrix estimation. 

---

class: animated, slideInRight
# Divide-and-Conquer
- Basic idea
  - divide the high-dimensional data into low dimensional subproblems
  - solve the subproblems in parallel using existing MCMC samplers 
  - combine the estimates to produce a global estimate of the covariance matrix 

- Divide-and-conquer approaches in the literature focus on tackling *large `\(n\)`* problems where the data are assumed to be independent and identically distributed &lt;span style="color:blue"&gt;(Mackey et al. 2011, Zhang et al. 2013, Minsker et al. 2014, Cheng &amp; Shang 2015)&lt;span&gt;
 
&gt; We split dimension `\(p\)` across the subproblems.

???

At a high level, our approach

-randomly divides the high dimensional data into low-dimensional subproblems
-solves these subproblems in parallel using existing MCMC samplers
- combines the solutions to produce a global estimate of the covariance matrix. 

The existing DNC methods in the literature focus on large `\(n\)` where subsetting occurs in the number of samples. In our framework, we split across dimensions so that each subproblem has access to all the samples. 

---

class: animated, fadeInDown
# Divide step
Randomly partition `\(y_i \in \mathbb{R}^p\)` 
`$$y_i \longrightarrow (y_i^{(1)}, \ldots, y_i^{(g)}), \quad i = 1,\ldots,n$$`
so that `\(y_i^{(m)} \in \mathbb{R}^{p_g}\)` where `\(p_g = p/g\)` and `\(m = 1,\ldots,g\)`.
&lt;img src="divide.png" width="50%" style="display: block; margin: auto;" /&gt;

???
In the first step, we randomly partition the `\(p-\)`dimensional vectors into `\(p_g-\)`dimensional sub vectors. For simplicity, we will assume that `\(p\)` is a multiple of `\(g\)`. 

---

# Fit Step

On machine `\(m\)`,
`$$y_i^{(m)} = \Lambda^{(m)}\;\eta^{(m)}_{i} + \epsilon_i^{(m)}, \quad i = 1, \ldots, n,$$` 
where `\(\Lambda^{(m)} \in \mathbb{R}^{p_g \times k_g}\)`, `\(\eta_i^{(m)} \in \mathbb{R}^{k_g}\)` and `\(\epsilon_i^{(m)} \in \mathbb{R}^{p_g}\)`. 
--

`$$\underbrace{\Pi(\Lambda^{(m)}, \eta^{(m)}, \Omega^{(m)} \mid y)}_{posterior} \propto \prod\limits_{i=1}^{n}\underbrace{\text{N}(y_i^{(m)} \mid \Lambda^{(m)}\eta_i^{(m)}, \Omega^{(m)})}_{Likelihood} \underbrace{\pi(\Lambda^{(m)})\pi(\eta_i^{(m)})\pi(\Omega^{(m)})}_{priors}$$`

--

Run the Gibbs sampler to obtain posterior estimate of `\(\Sigma^{(m)} \in {\mathbb{R}}^{{p_g \times p_g}}\)` via 

`$$\hat\Sigma^{(m)} = \hat\Lambda^{(m)}\hat\Lambda^{(m)T} + \hat\Omega^{(m)}$$`

???

Machine `\(m\)` is supplied with n independent copies of the divided data. 

On machine `\(m\)`, we regress `\(y^{(m)}\)` on `\(k_g\)`dimensional latent vectors where `\(k_g = k/g\)`. 

Note here that we have divided the factors across machines too in order to gain further computational speedups and savings. 

Inferences are carried out, as before, by drawing random samples from the posterior distribution using the Gibbs sampler. 

These samples can be used to obtain an estimator for `\(\hat\Sigma^{(m)}\)`

---

#### Computational Gains


| Model       | Complexity      |
|---------------|:-------------:|
| Original Fit step     | `\({O}(k^3 + npk + nk^2 + pk^2)\)` |
| Fit step in DNC |  `\({O}((k/g)^3 + n(pk/g^2) + n(k/g)^2 + (pk^2/g^3))\)`     | 

--

**How to combine estimates? How to Conquer?**
--
&lt;img src="inducingcorr2.jpeg" width="100%" /&gt;

???
How do we combine estimates to form a global estimate of the covariance matrix? In other words, how do we do the conquer step? 

Say we obtain estimates from 3 machines. If we assume them to be independent, we will lose the inherent dependence structure in the original p-dimensional observations and get something like in the right picture. 

Next, I will present one solution to overcome this problem. 

---
class: animated, fadeInDown
# Inducing Dependence

Consider the hierarchical model
`$$\eta_i^{(m)} = \sqrt{\rho}X_i + \sqrt{1 - \rho}Z_i^{(m)}$$`
where 
  - `\(X_i \sim N(0, I_{k_g})\)` : shared component 
  
  - `\(Z_i^{(m)} \sim N(0, I_{k_g})\)` : machine-specific component (not shared)
  
  - `\(0 &lt; \rho &lt; 1\)`   


&gt; If `\({\text{Cov}}\big(Z_i^{(m)},Z_i^{(m')}\big) = 0\)`, then  `\(\text{Cov}\big(y_i^{(m)},y_i^{(m')}\big) = \rho\Lambda^{(m)}\Lambda^{(m')}\)`.
  
???
We augment the factors over two components. One of them is shared while the other one is not shared. The shared component allows for a modeling framework wherein the estimates obtained from different machines are correlated with each other. 

It's quite straight forward to show that, if the components that are not shared are not correlated, then the correlation between observed vectors on two different machines takes the form ... 

Using this hierarchical model, we show that global estimator for the covariance matrix obtained from our approach takes the following form. 

---
class:animated,fadeInDown
#Conquer Step

The covariance matrix is given by `$$\Sigma = \Lambda_B \Lambda_B^{T} + (1 - \rho)\bigg(\mbox{diag}\{\Lambda^{(1)}\Lambda^{(1)T}, \ldots, \Lambda^{(g)}\Lambda^{(g)T}\} - \Lambda_B\Lambda_B^{T}\bigg) + \check\Omega$$` 

where `\(\Lambda_B^T = (\Lambda^{(1)}, \ldots, \Lambda^{(g)}) \in \mathbb{R}^{\frac{k}{g} \times p}\)` and `\(\check\Omega = \text{diag}(\Omega^{(1)}, \ldots, \Omega^{(m)})\)`. 

#### Notes
- To sample from `\(\eta^{(m)} \sim N_k(\mu_\eta,\Sigma_\eta \mid \Lambda^{(m)}, \Omega^{(m)})\)`, draw samples from
  - `\(X \sim N_k(\mu_\eta,\Sigma_\eta \mid \Lambda^{(m)}, \Omega^{(m)}, Z^{(m)})\)`
  - `\(Z^{(m)} \sim N_k(\mu_\eta,\Sigma_\eta \mid \Lambda^{(m)}, \Omega^{(m)}, X)\)` 

---

# Conquer Step: Illustration

For `\(g = 3\)`, an estimate of `\(\Sigma\)` is given by 
`$$\Sigma = \begin{pmatrix}\Lambda^{(1)}\Lambda^{(1)T} + \Omega^{(1)} &amp; \rho\Lambda^{(1)}\Lambda^{(2)^T} &amp; \rho\Lambda^{(1)}\Lambda^{(3)^T}\\ 
\rho\Lambda^{(2)}\Lambda^{(1)T} &amp;              \Lambda^{(2)}\Lambda^{(2)T} &amp; \rho\Lambda^{(2)}\Lambda^{(3)^T} \\ \rho\Lambda^{(3)}\Lambda^{(1)^T} &amp; \rho\Lambda^{(3)}\Lambda^{(2)^T} &amp; \Lambda^{(3)}\Lambda^{(3)T} + \Omega^{(3)}\end{pmatrix}$$`

--

&lt;img src="inducingcorr3.jpeg" width="100%" /&gt;

---

class: center,middle, animated, slideInRight
#Synthetic Data Analysis

Experiments performed on simulated data using [Flux](https://arc-ts.umich.edu/flux/)
available at [University of Michigan](https://umich.edu/). 


???
Next I will show you the advantage of our approach through numerical experiments on simulated data. All experiments were performed on flux which is a linux-based high performance computing cluster at University of Michigan.

---

class:animated, fadeInDown

# Setup

`$$y_i = \Lambda \eta_i + \epsilon_i, \quad i = 1,\ldots,n$$`
- `\(\epsilon \sim N(0,\Omega), \;\; \Omega = \text{diag}(\sigma_1^2, \ldots, \sigma_p^2), \;\; \sigma^{-2} \sim \Gamma(1,0.25)\)`

- `\(\Lambda_h \in \mathbb{R}^p, \; h = 1,\ldots,k\)` is such that `\(\lvert{\Lambda_h}\rvert = \log{}(p)\)`

- `\(\Lambda_{jh} \sim U(0.1,3)\)`

- `\(n = 100\)` for all choices of `\(\{p,k\}\)` and `\(g\)`

- Number of replications = 100

- Comparison in terms of 
  - operator norm `\(\|\cdot\|_2\)` where `\(\|A\|_2 = s_{\text{max}}(A)\)`,
  - frobenius norm `\(\|\cdot\|_F\)` where `\(\|A\|_F = \sqrt{\text{tr}(A^TA)}\)`
  - computation time, in minutes, per replication. 

???

- We generate data from a factor model where `\(\Omega = \sigma^2\mathrm{I}\)`. 
- Each column of the matrix has at most these many non-zero entries. These entries occur at random and are drawn from `\(U(0.1,3)\)`. 
- Number of samples is set to 100 for all experiments and we explore our method for different values of (dimensions, factors) and groups. 
- Accuracy is measured in terms of operator and Frobenius norms. 
- Computational efficiency is measured in terms of computation time per replication. 
---

class: animated, fadeInDown
### Statistical Accuracy vs Computational Gain 

- `\((p,k) = \{(252,6), (504,12), (1008,24), (2016,36)\}\)` 
- `\(g = \{1,3,6\}\)`

.pull-left[
&lt;img src="errR.jpg" width="80%" /&gt;
]
.pull-right[
&lt;img src="timeR.jpg" width="80%" /&gt;

]

???
General trends:
1. As `\(p\)` and `\(k\)` increase, the errors and times increase. 
2. So let's fix `\(p\)` and `\(k\)` and examine the plots. 
3. As we increase `\(g\)`, the errors increase but not by so much. The times decrease significantly. 
4. Let's fix `\(p\)` and `\(k\)` to high values. And we can see the advantage of the divide and conquer framework. For p = 2016, fitting a factor model on a single machine takes 4 hours. Setting `\(g = 3\)`, this job can be done in half the time i.e. about 2 hours. Setting it to 6, the job can be done in an hour in exchange in a very small loss in statistical accuracy. 


---

class: animated, fadeInDown
### Sensitivity to Random splitting

.pull-left[
&lt;div class="figure"&gt;
&lt;img src="froerr-RS-p252.jpeg" alt="Frobenius norm errors for p=1008" width="80%" /&gt;
&lt;p class="caption"&gt;Frobenius norm errors for p=1008&lt;/p&gt;
&lt;/div&gt;
]
.pull-right[
&lt;div class="figure"&gt;
&lt;img src="operr-RS-p1008.jpeg" alt="Operator norm errors for p=1008" width="80%" /&gt;
&lt;p class="caption"&gt;Operator norm errors for p=1008&lt;/p&gt;
&lt;/div&gt;

]

???

The results on the previous slide are obtained from 100 replications which means every single time new data was generated and the divide and conquer was appplied to the data. Is it possible that I got lucky with the random split in the divide step every time and got these optimistic results? 

Here I do the experiment 100 times but this time I fix my data set and change my random splits every single time and display the results. 

Now these figures do indicate that there are some splits that are more favourable than others but overall our method is quite robust to random splitting in the divide step.

---

class: animated, fadeInDown
### Sensitivity to Random splitting
.pull-left[
&lt;div class="figure"&gt;
&lt;img src="froerr-RS-p1008.jpeg" alt="Frobenius norm errors for p=2016" width="80%" /&gt;
&lt;p class="caption"&gt;Frobenius norm errors for p=2016&lt;/p&gt;
&lt;/div&gt;
]
.pull-right[
&lt;div class="figure"&gt;
&lt;img src="operr-RS-p2016.jpeg" alt="Operator norm errors for p=2016" width="80%" /&gt;
&lt;p class="caption"&gt;Operator norm errors for p=2016&lt;/p&gt;
&lt;/div&gt;

]

---
class: animated, fadeInDown
### Results

- Compare [Divide and Conquer](https://github.com/gautam-sabnis/A-Divide-and-Conquer-Strategy-for-High-Dimensional-Bayesian-Factor-Models) (**DNC**) with [Principal Orthogonal Complement Thresholding](https://cran.r-project.org/web/packages/POET/index.html) (**POET**) and [Bayesian Factor Regression Modeling](https://www2.stat.duke.edu/~mw/mwsoftware/BFRM/index.html) (**BFRM**) in terms of statistical accuracy.

- `\(n = 100\)`, `\((p,k) = \{(252,6), (1008, 24), (2016,36)\}\)`, `\(g = \{3,6\}\)`

.pull-left[
&lt;img src="opcomp2.jpg" width="80%" /&gt;
]
.pull-right[
&lt;img src="timecomp2.jpg" width="80%" /&gt;
]
???
I compare DNC with two very popular methods for covariance matrix estimation based on factor models in the literature. POET is an optimization free method that proposes an estimator of `\(\Sigma\)` based on PCA. BFRM is a comprehensive implementation of sparse statistical models in a Bayesian framework suited to many areas of applications in cancer genomics. Here I use it to estimate the covariance matrix.

---

.pull-left[
&lt;img src="err4032.jpg" width="100%" /&gt;
]
.pull-right[
&lt;img src="time4032.jpg" width="100%" /&gt;
]
--

- **POET** runs into memory issues for `\(p = 4032\)`. 

--

p | k | Method | Error | Time  
--- | --- | --- | --- | --- | --- 
`\(10^4\)`| 100 | `\({\text{DNC}}_1\)` | `\({\color{red}{\text{Fail}}}\)` | `\({\color{red}{\text{Fail}}}\)` 
`\(10^4\)`| 100 | POET | `\({\color{red}{\text{Fail}}}\)` | `\({\color{red}{\text{Fail}}}\)` 
`\(10^4\)`| 100 | BFRM | `\({\color{red}{\text{Inaccurate}}}\)` | `\({\color{red}{\text{Inaccurate}}}\)` 
`\(10^4\)`| 100 | `\({\text{DNC}}_{10}\)` | 66.28 (0.09) | 5 days 
`\(10^4\)`| 100 | `\({\text{DNC}}_{20}\)` | 69.55 (0.16) | 3 days 

   

---

# Application to Thyroid Gene Expression Data

Dysregulation of gene expressions in the thyroid is associated with differnt types of metabolic and autoimmune diseases and thyroid cancer. 

**Data**

-	17,908 gene expressions of thyroid tissue samples from 278 donors
- Access to additional information such as time of death and sample collection 

**Goal** 

Estimate the covariance matrix in order to understand the second order structure among these gene expressions i.e. how they co-vary.

---

class: animated, fadeInDown
# Analysis
- Data matrix: `\(Y \in \mathbb{R}^{278 \times 17,908}\)`
- We estimate the covariance matrix to obtain `\(\hat\Sigma\)` using `\(g = 11\)` with `\(p_g = 1,628\)` genes. 
- We ran the Gibbs sampler for `\(15,000\)` iterations with `\(5,000\)` burn-in and collected every first sample after burn-in to thin the chain. 
- The number of factors were set to `\(k = 110\)`. 
- The 10 estimated factors across 11 groups are nearly identical, highlighting that each group finds similar estimates of the latent subspace despite using disjoint groups of genes. 

&lt;img src="thyroid1.jpg" width="60%" /&gt;

---
# Analysis 

- Entries in `\(\Lambda\)` were thresholded to cluster genes within factors

-  A Gene ontology enrichment analysis `\({\color{blue}{\text{(Eden et al., 2009)}}}\)` showed that 
  -	many of the clustered genes in the first factor are related to cellular death processes
  -	clustered genes contained in many other factors are involved in cellular metabolic processes (factors 3-6, 15, 22)

Further analysis showed correlation of factors with ischemic time. This along with enrichment of genes involved in processes related to cellular metabolism and death suggested that covariation in the data is driven by the rapid decay of tissue samples postmortem. 


---
class: animated, fadeInDown
# Summary

- A Divide and Conquer strategy is proposed to accelerate posterior inference for high-dimensional covariance matrix estimation in Bayesian sparse factor models is proposed. 

- The approach is different from the existing divide and conquer strategies in that each subproblem includes all `\(n\)` samples and, instead, splits problem across dimensions.  

- Numerical studies demonstrate the computational speedups offered at the cost of minimial loss is accuracy. 

- Application to gene expression data reveals certain interesting biological processes but more research is needed to understand what is jointly regulating the expressed genes. 

- Theoretical guarantees in terms of some common functionals of the covariance matrix. 

???

Obviously I didn't cure cancer. But I made a small breakthrough. 

---

class: inverse, animated, slideInRight
# Acknowledgements

- Debdeep Pati (Texas A&amp;M University)
- Barbara Engelhardt (Princeton University)
- Natesh Pillai (Harvard University)

---

class: inverse, center, middle, animated, slideInRight

# Thanks for your patience and attention! 


---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true,
"header": "\\usepackage{xcolor}"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
